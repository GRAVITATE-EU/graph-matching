/////////////////////////////////////////////////////////////////////////
//
// (c) Copyright University of Southampton IT Innovation, 2015
//
// Copyright in this software belongs to IT Innovation Centre of
// Gamma House, Enterprise Road, Southampton SO16 7NS, UK.
//
// This software may not be used, sold, licensed, transferred, copied
// or reproduced in whole or in part in any manner or form or in or
// on any media by any person other than in accordance with the terms
// of the Licence Agreement supplied with the software, or otherwise
// without the prior written consent of the copyright owners.
//
// This software is distributed WITHOUT ANY WARRANTY, without even the
// implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR
// PURPOSE, except where stated in the Licence Agreement supplied with
// the software.
//
// Created By : Jessica Rosati
// Created Date : 2017/06/26
// Created for Project : GRAVITATE
//
/////////////////////////////////////////////////////////////////////////
//
// Dependencies : NONE 
//
/////////////////////////////////////////////////////////////////////////
package ITinnov.semantic_matching;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.FileReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStreamWriter;
import java.io.UnsupportedEncodingException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Properties;

import com.hp.hpl.jena.query.Dataset;
import com.hp.hpl.jena.query.Query;
import com.hp.hpl.jena.query.QueryExecution;
import com.hp.hpl.jena.query.QueryExecutionFactory;
import com.hp.hpl.jena.query.QueryFactory;
import com.hp.hpl.jena.query.QuerySolution;
import com.hp.hpl.jena.query.ResultSet;
import com.hp.hpl.jena.query.ResultSetFactory;
import com.hp.hpl.jena.rdf.model.Model;
import com.hp.hpl.jena.tdb.TDBFactory;
import com.hp.hpl.jena.util.FileManager;
/*
 * This class defines the walks/sequences on the RDF graph.
 * It writes the walks into a file.
 * Such walks will then be used to train the word2vec model (in rdf_walks2vec_app.py)
 */

public class Graph_embedding_app {
	static Properties properties;
	private static Dataset dataset;
	private static Model tdb; //com.hp.hpl.jena.rdf.model.Model
	private static OutputStreamWriter stream_writer;// to write the output file with walks

	static int numberWalks; // number of walks for each seed node (starting node of the random query)
	static int depthWalk; // depth of the walk
	static boolean literals; // considering literals or not 
	private String filterForLiterals = literals ? "" : "FILTER(!isLiteral(?o))";

	/**
	 * the query to extract direct links
	 */
	private static String oneHopQuery;
	/**
	 * the query to extract walks (with depth depthWalk)
	 */
	private static String walkQuery;
	/**
	 * a dictionary with entries <RDF node, int_id> (see file dictionary_for_starsDefinition.txt generated by Star_distance_app.java) 
	 */
	private static HashMap<String, Integer> dictionary;
	
	/*
	Gets the parameters values specified in the config.properties file
	Input: a string representing the path of config.properties file with parameters 
	 */
	static private void getParamatersValues(String strPath) {
		ParameterValues gettingParameterValues = new ParameterValues();
		properties = gettingParameterValues.loadParamsFromFile(strPath);
	}

	/*
	Reads the dictionary created by Star_distance_app.java, where each RDF node is associated to an int.
	Input: the path of the dictionary (dictionary_for_starsDefinition.txt) created by Star_distance_app.java. 
	 */
	private static void read_dictionary_node_counter(String path_dictionary_for_starsDefinition) {
		// TODO Auto-generated method stub
		FileInputStream fis;
		InputStreamReader isr = null;
		try {
			fis = new FileInputStream( path_dictionary_for_starsDefinition);
			isr = new InputStreamReader( fis, java.nio.charset.StandardCharsets.UTF_8 );

		}
		catch (FileNotFoundException e) {
			// TODO Auto-generated catch block
			System.out.println("FileNotFoundException reading "+path_dictionary_for_starsDefinition);
			System.out.println("Run Star_distance_app first");
			e.printStackTrace();
		}

		BufferedReader in = new BufferedReader(isr);
		String line;
		String[] line_splitted;
		try {
			while ((line = in.readLine()) != null) {
				line_splitted = line.split("\t");
				if( line_splitted.length > 1 ) {
					dictionary.put( line_splitted[1].replace(" ", ""), Integer.valueOf(line_splitted[0]) );
				}
			}
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
	}
	
	/*
	Load the .ttl file into a TDB model; Generates the walks on the RDF graph; Writes the walks into a .txt file.
	See "strPathOfOutputFileForWalks" for file where walks are written
	See "strPathOfOutputFileForConstructQuery" for .ttl file loaded into a TDB and then queried   
	 */
	Graph_embedding_app() throws IOException {
		// OutputStreamWriter for the file where walks are written
		try {
			stream_writer = new OutputStreamWriter(
				new FileOutputStream( properties.getProperty("strPathOfOutputFileForWalks"), false ),
				java.nio.charset.StandardCharsets.UTF_8 );
		} catch (FileNotFoundException e) {
			e.printStackTrace();
		}
		// Loading the .ttl file into a TDB model
		dataset = TDBFactory.createDataset(properties.getProperty("strPathTDBrepoConstruct"));
		tdb = dataset.getDefaultModel();
		try {
			//File f = new File(properties.getProperty("strPathOfOutputFileForConstructQueryUPDATED"));
			File f = new File(properties.getProperty("strPathOfOutputFileForConstructQueryUPDATED"));
			if (!f.exists()) {
				// use the original version of the triples file, because the updated version (resulting from the clustering of literal) of the triples file has not been found
				FileManager.get().readModel(tdb, properties.getProperty("strPathOfOutputFileForConstructQuery"),"TTL");
			} else {
				// use the updated version of the triples file
				// FileManager.get().readModel(tdb, properties.getProperty("strPathOfOutputFileForConstructQueryUPDATED"),"TTL");
				FileManager.get().readModel(tdb, properties.getProperty("strPathOfOutputFileForConstructQueryUPDATED"),"TTL");
			}
		} catch (NullPointerException e) {
			System.out.println("NullPointerException: Check file for strPathOfOutputFileForConstructQuery in config.properties");
			e.printStackTrace();
			throw(e);
		} catch (Exception e) {
			System.out.println("Error reading file "+properties.getProperty("strPathOfOutputFileForConstructQueryUPDATED"));
			e.printStackTrace();
		}
		// walks generation
		generateWalksOnGraph();
		dataset.close();
		try {
			stream_writer.close();
		} catch (IOException e) {
			e.printStackTrace();
		}
	}

	/*
	Reading the artefacts URIs
	 */
	private static List<String> selectUsefulSeedEntities() {
		ArrayList<String> root_entities = new ArrayList<String>();
		BufferedReader br;
		try {
			FileInputStream fis = new FileInputStream(properties.getProperty("strPathArtefactsURIs"));
			br  = new BufferedReader( new InputStreamReader(fis, java.nio.charset.StandardCharsets.UTF_8) );

			String strCurrentLine;
			String URI_item;
			while ((strCurrentLine = br.readLine()) != null ) {
				String[] splitted = strCurrentLine.split("\t");
				URI_item = splitted[0];
				root_entities.add(URI_item);
			}
			br.close();
		} catch (FileNotFoundException e) {
			// TODO Auto-generated catch block
			System.out.println("file with artefacts' URIs not found");
			e.printStackTrace();
		} catch (IOException e) {
			// TODO Auto-generated catch block
			System.out.println("IOException reading file with artefacts' URIs");
			e.printStackTrace();
		}
		return root_entities;
	}

	/*
	Generates walks on RDF graph (modelled in the TDB model).
	Two queries are run against TDB to define walks: walkQuery extracts (random) walks with a certain depth, oneHopQuery extracts direct linked resources (1-hop query)	 
	 */
	private void generateWalksOnGraph() throws IOException {
		// TODO Auto-generated method stub
		// generate the walk query
		walkQuery = SPARQLQueryBuilder.calcRandomWalkQuery(depthWalk, numberWalks, literals);
		//walkQuery = SPARQLQueryBuilder.calcRandomWalkQueryUpToDepth(depthWalk, numberWalks, literals);

		//        System.out.println("SELECTING all entities from model");
		//        List<String> root_entities = selectAllEntities(limit);

		//only the artefacts are considered as root for random walks
		System.out.println("SELECTING useful entities from model");
		List<String> root_entities = selectUsefulSeedEntities();

		System.out.println("Total number of entities to process: "
				+ root_entities.size());

		for (String entity : root_entities) {
			executeQueriesforEntity(entity);
		}
	}

	/*
	Executes two queries against TDB to define walks: walkQuery extracts random walks with a certain depth, oneHopQuery extracts direct linked resources (1-hop query)	 
	 */
	private void executeQueriesforEntity(String entity) throws IOException {
		entity = entity.replace("\"", "");
		// get all the walks
		String queryStr = walkQuery.replace("$ENTITY$",  entity );
		executeQuery(queryStr, entity);

		// get all the direct properties
		oneHopQuery = "SELECT DISTINCT ?p ?o WHERE {<" + entity + ">" + " ?p ?o . " +   //(str(?o) as ?oStr)
				filterForLiterals +
				"}";
		executeQuery(oneHopQuery, entity);
	}

	/*
	Executes a query against TDB and writes the walk generated into the output file.
	Input: the query to execute
	Input: the entity to use as root (subject) in the query	 
	 */
	public void executeQuery(String queryStr, String entity) throws IOException {
		Query query = QueryFactory.create(queryStr);
		QueryExecution qe = QueryExecutionFactory.create(query, tdb);
		ResultSet resultsTmp = qe.execSelect();
		ResultSet results = ResultSetFactory.copyResults(resultsTmp);
		qe.close();
		while (results.hasNext()) {
			QuerySolution result = results.next();
			String singleWalk = entity + " ";//"str"+dictionary.get(entity)
			// construct the walk
			for (String var : results.getResultVars()) {
				try {
					// we are using the integer corresponding to RDF resource if available in dictionary
					// if literal
					if (result.get(var) != null){ 
					String value = result.get(var).toString();
					if (result.get(var).isLiteral()) {
						String val = result.getLiteral(var).getString();
						val = val.replace(" ", "");	
						if(dictionary.containsKey(val)){
							singleWalk += "str"+dictionary.get(val)+" ";}//"str"+dictionary.get(val)+" "+"str"+dictionary.get(val) + " ";
						else singleWalk += val + " ";
					} else if (dictionary.containsKey(value)) {// if not literal
						singleWalk += "str"+dictionary.get(value) + " ";
					} else{singleWalk += value + " ";}
				}} catch (Exception e) {
					e.printStackTrace();
				}
			}

			// do not do any URI replacement here - its done in graph construct app
			stream_writer.write(
				singleWalk.substring(0, singleWalk.length() - 1)
				+ "\n" );

			/* OLD
			stream_writer.write(singleWalk.substring(0, singleWalk.length() - 1)   // replace very common prefixes
					.replace("http://collection.britishmuseum.org/id/object/", "bmo:")
					.replace("http://collection.britishmuseum.org/id/thesauri/","bmt:")
					.replace("http://collection.britishmuseum.org/id/person-institution/","bmp:") + "\n");
			*/
		}
	}

	public static void main(String[] args) throws IOException {
		getParamatersValues("config.properties");
		numberWalks = Integer.valueOf(properties.getProperty("numberWalks"));
		depthWalk = Integer.valueOf(properties.getProperty("depthWalk"));
		literals = Boolean.valueOf(properties.getProperty("literals"));
		dictionary = new HashMap<String, Integer>();
		read_dictionary_node_counter("dictionary_for_starsDefinition.txt");
		Graph_embedding_app g = new Graph_embedding_app();
	}
}
